<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="true">
  <meta name="msvalidate.01" content="true">
  <meta name="yandex-verification" content="true">
  <meta name="baidu-site-verification" content="true">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.lirui.pub","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
     crossorigin="anonymous"></script>


  <meta name="baidu-site-verification" content="zqFkvU5IKZXfWGBL" />

  <meta name="description" content="一文滤清预训练语言模型发展脉络 前言 1. BERT 之前 1.1 从 Word Embedding 到 ELMO 1.2 Fine-tuning pretraining：  GPT 的诞生 4. 预训练新时代：BERT   2. BERT 之后 1. 预训练 + 知识图谱 2. 预训练 +  自然语言生成 3. 预训练 + 多任务学习 4. 改进语言模型 5. 预训练 + 中文领域 6. 预">
<meta property="og:type" content="article">
<meta property="og:title" content="一文缕清预训练语言模性发展脉络">
<meta property="og:url" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E4%B8%80%E6%96%87%E6%BB%A4%E6%B8%85%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/index.html">
<meta property="og:site_name" content="Earyant的技术博客">
<meta property="og:description" content="一文滤清预训练语言模型发展脉络 前言 1. BERT 之前 1.1 从 Word Embedding 到 ELMO 1.2 Fine-tuning pretraining：  GPT 的诞生 4. 预训练新时代：BERT   2. BERT 之后 1. 预训练 + 知识图谱 2. 预训练 +  自然语言生成 3. 预训练 + 多任务学习 4. 改进语言模型 5. 预训练 + 中文领域 6. 预">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-03-12T15:33:22.000Z">
<meta property="article:modified_time" content="2022-03-12T15:33:22.000Z">
<meta property="article:author" content="Earyant">
<meta property="article:tag" content="机器学习知识整理">
<meta property="article:tag" content="深度学习自然语言处理">
<meta property="article:tag" content="预训练语言模型">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E4%B8%80%E6%96%87%E6%BB%A4%E6%B8%85%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>一文缕清预训练语言模性发展脉络 | Earyant的技术博客</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?7b631c63a156a1da87aae10873b41f51";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Earyant的技术博客" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Earyant的技术博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到Earyant的技术博客，在这里我将与你分享新技术。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">88</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">19</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">339</span></a>

  </li>
        <li class="menu-item menu-item-相册">

    <a href="/gallery/" rel="section"><i class="fa fa-camera-retro fa-fw"></i>相册</a>

  </li>
        <li class="menu-item menu-item-留言板">

    <a href="/guestbook/" rel="section"><i class="fa fa-comment fa-fw"></i>留言板</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>


        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
            crossorigin="anonymous"></script>
        <ins class="adsbygoogle"
            style="display:block; text-align:center;"
            data-ad-layout="in-article"
            data-ad-format="fluid"
            data-ad-client="ca-pub-8990335822972779"
            data-ad-slot="3743679245"></ins>
        <script>
            (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      </div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/earyantLe" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E4%B8%80%E6%96%87%E6%BB%A4%E6%B8%85%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me/me2.png">
      <meta itemprop="name" content="Earyant">
      <meta itemprop="description" content="个人技术博客，分享开发中遇到的问题，以及想学的新技术，会持续更新，可以订阅rss。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Earyant的技术博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          一文缕清预训练语言模性发展脉络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-12 23:33:22" itemprop="dateCreated datePublished" datetime="2022-03-12T23:33:22+08:00">2022-03-12</time>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

  
	<div>
      <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
          crossorigin="anonymous"></script>
      <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-8990335822972779"
          data-ad-slot="3743679245"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
  </div>
  

    
    
    
    <div class="post-body" itemprop="articleBody">

      


        <!-- toc -->
<ul>
<li><a href="#一文滤清预训练语言模型发展脉络">一文滤清预训练语言模型发展脉络</a><ul>
<li><a href="#前言">前言</a></li>
<li><a href="#1-bert-之前">1. BERT 之前</a><ul>
<li><a href="#11-从-word-embedding-到-elmo">1.1 从 Word Embedding 到 ELMO</a></li>
<li><a href="#12-fine-tuning-pretraining-gpt-的诞生">1.2 Fine-tuning pretraining：  GPT 的诞生</a></li>
<li><a href="#4-预训练新时代bert">4. 预训练新时代：BERT</a></li>
</ul>
</li>
<li><a href="#2-bert-之后">2. BERT 之后</a><ul>
<li><a href="#1-预训练-知识图谱">1. 预训练 + 知识图谱</a></li>
<li><a href="#2-预训练-自然语言生成">2. 预训练 +  自然语言生成</a></li>
<li><a href="#3-预训练-多任务学习">3. 预训练 + 多任务学习</a></li>
<li><a href="#4-改进语言模型">4. 改进语言模型</a></li>
<li><a href="#5-预训练-中文领域">5. 预训练 + 中文领域</a></li>
<li><a href="#6-预训练-精细调参">6. 预训练 + 精细调参</a></li>
<li><a href="#7-预训练-基础单元">7. 预训练+ 基础单元</a></li>
</ul>
</li>
<li><a href="#最后">最后</a></li>
<li><a href="#reference">Reference</a><ul>
<li><a href="#1-ar-与-ae-语言模型">1. AR 与 AE 语言模型</a></li>
</ul>
</li>
<li><a href="#bert-诞生之前">BERT 诞生之前</a></li>
<li><a href="#gpt-系列">GPT 系列</a></li>
<li><a href="#bert-系列">BERT 系列</a></li>
<li><a href="#多任务学习">多任务学习</a></li>
<li><a href="#中文领域">中文领域</a></li>
<li><a href="#融入知识">融入知识</a></li>
<li><a href="#多语言">多语言</a></li>
<li><a href="#模型压缩">模型压缩</a></li>
<li><a href="#文本生成">文本生成</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->
<span id="more"></span>
<h1><span id="一文滤清预训练语言模型发展脉络">一文滤清预训练语言模型发展脉络</span></h1><h2><span id="前言">前言</span></h2><p>预训练语言模型的诞生已经3年多了，其衍生的各个子领域可谓是百花齐放，这篇文章对我这几年看过的预训练语言模型方面的 paper 进行一个梳理，对于每个子领域有遗漏的欢迎大家评论补充。</p>
<p>这篇文章会以月为单位更新，拭目以待。</p>
<h2><span id="1-bert-之前">1. BERT 之前</span></h2><h3><span id="11-从-word-embedding-到-elmo">1.1 从 Word Embedding 到 ELMO</span></h3><p>考虑到词向量不能解决词的多义性问题，在 ELMO 之前，我们往往采用双向 LSTM 来减轻这种问题，但这毕竟治标不治本，对于大数据集好说， 深层双向 LSTM 的确能够很好的缓解这种问题，但对于小数据集，往往没啥效果。</p>
<p>为了解决这种多义性问题，ELMO 在训练语言模型时采用双向 LSTM 。 不同层的 LSTM 能够把握不同粒度和层级的信息，比如浅层的 LSTM 把握的是单词特征， 中层的 LSTM 把握 句法 特征， 深层的 LSTM 把握语义特征， 对于不同的任务来说， 不同的特征起到了不同的作用。 </p>
<p>举例来说： 文本分类问题为何 ELMO 与 BERT 所起到的作用与 Word2Vec 相差无几，这就是因为对于分类问题来说， n-gram 信息起到很大的作用，而这本质就是单词特征； 但对于阅读理解领域， ELMO 与 BERT 就能大幅提高模型效果，这也是因为 语法与语义特征对于阅读理解这种深层次问题是十分重要的。</p>
<p>ELMO 在迁移到下游任务时，会将不同层的特征采用<strong>加权求和</strong>的方式来获得每个词的最终表示。</p>
<p>事实证明， ELMO 的确解决了多义性问题， 词性也能对应起来了。</p>
<p>但， ELMO 的缺点也十分明显：</p>
<ul>
<li><strong>LSTM 特征抽取能力远弱于 Transformer ， 并行性差</strong></li>
<li><strong>拼接方式双向融合特征融合能力偏弱</strong></li>
</ul>
<h3><span id="12-fine-tuning-pretraining-gpt-的诞生">1.2 Fine-tuning pretraining：  GPT 的诞生</span></h3><p>GPT 虽然不是第一个预训练语言模型，但它的出现更具<strong>开创意义</strong>。其特点很明显：</p>
<ul>
<li>采用<strong>单向 Transformer</strong> 作为特征抽取器</li>
<li>采用二阶段： 预训练 + 微调  来适配下游任务</li>
</ul>
<p>GPT 1.0 与 GPT 2.0 的出现说明了一下几点：</p>
<ul>
<li>高质量，大规模的预训练数据集是提升性能的根本</li>
<li>深层的 Transformer 模型具有更强的表示能力</li>
</ul>
<p>至少，从目前为止， 业界还没有探索到数据与模型的极限，即仅仅堆数据，加深模型这条路，还没有走完。</p>
<h3><span id="4-预训练新时代bert">4. 预训练新时代：BERT</span></h3><p>GPT 虽然很强，但由于其基于 AR 模型且目前很多排行榜都是基于<strong>自然语言理解</strong>的，因此， GPT 在这方面无法与 BERT 的表现相抗衡。但 GPT 在生成方面是 BERT 无法比拟的， 就问你BERT： 会编故事吗？</p>
<p>BERT 主要分为两大部分： <strong>Masked LM</strong> 与 <strong>NSP</strong> (Next Sentence Prediction)。</p>
<p>BERT 由于其采用 AE 模型，MASK 操作所带来的缺陷依旧存在：</p>
<ul>
<li>预训练与微调阶段不匹配的问题，这点 BERT 提供了一个策略来减轻该问题</li>
<li>Mask 掉的  token 之间关系被忽略的问题</li>
</ul>
<p>此外，由于数据量，模型都十分大，如果每次只 mask 一个token，那么整个训练过程将变得极为漫长， 文章采用 mask 15% 的操作，是一个经验性的选择，是对模型训练效果与训练时长做出了一个权衡。</p>
<p>至于 NSP 任务，事实证明其在句子关系上的确起到了一定的作用，对于某些任务的确有帮助，但也有文章指出，其实用处不大，这点后面会详细讨论。</p>
<h2><span id="2-bert-之后">2. BERT 之后</span></h2><p>BERT 之后，有诸多改进方案，无论是对语言模型进行改进，融合知识图谱进行改进，多任务学习+预训练语言模型等， 这些文章都具有很大的价值，且质量都很高，本节的目的是对最近的这些模型进行一个全面的总结，帮助人们理清思路。对此，我画了一个直观的优化图，如下图所示：</p>
<h3><span id="1-预训练-知识图谱">1. 预训练 + 知识图谱</span></h3><p>预训练诞生之后， 在自然语言理解领域的确获得了很大的提升，尤其是在阅读理解领域，完全超过了人类的表现，虽然这并不表示真正的智能，但依旧意味着，NLP 已经逐渐走向成熟。</p>
<p>随之而来的问题十分明显， 如何表示知识， 有没有一种方式能够利用<strong>大规模语料+预训练语言模型</strong>使得模型能够学习到知识，从而应用到下游任务中。相信这个课题将是接下来一个十分核心的热点， 百度和清华就这方面做出了探讨， 具体可参加： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/69941989">Bert 改进： 如何融入知识</a></p>
<p>百度的文章中提出通过 mask 掉实体来获取实体的表示， 可以肯定的是，这样是能够更好的表示实体信息，但对于实体关系的把握，我个人觉得存疑，这是因为 mask 操作往往不仅仅 mask 掉一个实体，那么被 mask 掉的实体之间的关系如何把握？</p>
<p>我个人觉得可以设计一个<strong>精巧的任务</strong>来验证实体之间的关系， 可以通过知识图谱来生成一个语料， 如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">谢霆锋是张柏芝的__。 </span><br></pre></td></tr></table></figure>
<p>我们来预测空白处的位置， 判断其是否为 <code>丈夫</code>， <code>前夫</code> 之类的词， 这点需要根据具体的知识图谱而定。</p>
<p>清华的那篇文章，其先编码实体与实体间关系信息为一个向量， 然后将向量融合如预训练语言模型中进行训练， 而实际的操作更为复杂，俺个人觉得，这条路恐怕不是正确的路，不符合大道至简的原则，且任务太多，反而会引入噪声（个人对知识图谱研究不深，只是直观感觉）。</p>
<p>目前来看，个人觉得百度的路是对的。</p>
<h3><span id="2-预训练-自然语言生成">2. 预训练 +  自然语言生成</span></h3><p>这部分包含两个课题： </p>
<ul>
<li><strong>如何将 BERT 用于生成任务</strong></li>
<li><strong>如何设计一个适合于生成任务的语言模型</strong></li>
</ul>
<p>前面在 AR 与 AE 模型中已经介绍过为何 BERT 不适用于生成任务中， 那么随之而来的问题就是，既然预训练语言模型在自然语言理解中如此成功，那么我们怎么将其迁移到自然语言生成中呢， 这是一个很大的问题，个人觉得还需要1年以上的时间发展才能出现类似 Bert 这样的突破。</p>
<p>我个人前期看了两篇文章，大致提了一下思路：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70663422">Bert 之后：预训练语言模型与自然语言生成</a></p>
<p>首先，对于第一个课题： <strong>如何将 BERT 用于生成任务。</strong> 从技术上来说， Encoder-Decoder 架构应该是首选的框架了， Encoder 输入原句子，Decoder 生成新句子，那么问题在于，Encoder 与 Decoder 如何表示？</p>
<p>对于 Encoder 端来说，我们只需要将 Bert 直接初始化就行；那么对于Decoder 端呢？ 也采用 Bert 初始化吗？ 要知道的是， Decoder 可是用来生成的， 如果你的 embedding 信息是通过 AE 模型训练得到的，那么生成效果估计会诡异的一批。 那么现在的问题就变成了， <strong>如何合理的初始化 Decoder 端的 embedding 信息呢？</strong></p>
<p>然后，我们再来谈谈第二个课题：<strong>如何设计一个适合于生成任务的语言模型。</strong> 目前从我看到的两篇文章中有两个思路：</p>
<ul>
<li>MASS 通过 mask 连续的<strong>一小段</strong>来试图即学习到理解知识，又学习到生成知识， 通过预测一段连续的 tokens 的确有助于提高模型生成方面的能力，但我个人觉得 mask 一小段信息所提升的生成能力十分有限， 且我认为这会影响到模型理解方面的能力。</li>
<li>UULM 就厉害了， 它涉及了一组语言模型： <strong>Unidirectional LM， Masked Bidirectional LM， Seq2Seq LM</strong>， 真的是有钱任性， 但这样直接堆语言模型的方式真的好吗？ 可以肯定的是， 不同语言模型的结合必然是接下来的一大趋势，但你这样直接堆是不是有点暴力啊，我个人感觉一般。</li>
</ul>
<p>那么，怎么去设计一个适合于生成任务的语言模型呢？ 我个人的想法在之前的博客提到了： 就人类而言， <strong>生成是基于理解的，而非独立的， 在大脑中， 理解与生成是两个区域， 先理解后生成，这才是正确的路。</strong> </p>
<p>因此，我个人觉得，接下来的一个思路应该是： <strong>理解的归理解，不断提高预训练语言模型在理解领域的表现， 对于生成，采用 Encoder-Decoder 框架。</strong> 在预训练的角度来说， 基于理解层面训练得到的模型， 然后分别初始化 Encoder-Decoder 端， 然后去预训练 Decoder 端的参数， Freeze/not Freeze Encoder 端的参数， 从而得到词在 Encoder 与 Decoder 的不同 Embedding， 然后再生成任务中的 Encoder-Decoder 中分别使用这两种 embedding。 </p>
<h3><span id="3-预训练-多任务学习">3.  预训练 + 多任务学习</span></h3><p>多任务学习就更好玩了，目前主要有两大代表： MT-DNN 与 ERNIE 2.0。</p>
<ul>
<li><p><strong>MT-DNN</strong> 又叫<strong>联合训练</strong>，其实就是将预训练语言模型用在多个任务中去接着预训练，从而提高模型泛化。具体来说，训练过程就是把所有数据合并在一起，每个batch只有单一任务的数据，同时会带有一个task-type的标志， 然后shuffle 之后进行训练。</p>
</li>
<li><p><strong>ERNIE</strong>  提出一个很好的思路： <strong>Continual Learning</strong>。 这点很有意思，就像人类做题一样， 它不像 MT-DNN 那样训练，而是这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">task1 --&gt; task1,task2 --&gt; task1, task2, task3</span><br></pre></td></tr></table></figure>
<p>即在训练后续任务时，前面的任务依旧要参与训练，主要是希望在学习后续任务时依旧记得前面任务的学习成果。</p>
</li>
</ul>
<p>我个人觉得 ERNIE 更符合我们人类的训练方式，不过具体的两种学习方式的表现还需要对比一下。 </p>
<p>回想我们人类的学习方式，其最初是专题训练，即每个 task 分别训练， 然后再进行总体训练，即所有 task 一起进行训练，然后发现自己的弱点，然后适当加强对某任务的训练，然后又进行总体训练，如此反复， 过程更像是这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(task1 or task 2 or task3)--&gt; (task1, task2), (task1, task3), (task2, task3) --&gt; (task1, task2, task3) --&gt; (task1 or task2 or task3) --&gt; ...</span><br><span class="line">专题训练 --&gt; 组合训练 --&gt; 总体训练 --&gt; 专题训练 --&gt; ...</span><br></pre></td></tr></table></figure>
<p>如果要保证训练新任务时不会过分忘记前面训练所得到的成果，似乎各个任务的训练样本比例以及训练时间更加重要。比如你做了一年的阅读理解，突然让你做单向选择，你答的也不会太好。</p>
<p>因此，我个人觉得， <strong>联合训练 + Continual Learning</strong> 是一个不错的思路。</p>
<p>不过我很疑惑的是，为何 7月份 有段时间 ERNIE 2.0 很火，我感觉它的创新性和各方面也就是 MT-DNN 一级别的啊，难道是宣传问题？ </p>
<h3><span id="4-改进语言模型">4. 改进语言模型</span></h3><p>要说起改进语言模型，当首推 <strong>XLNet</strong>， 毕竟前段时间也是刷了榜的，通过交换 token 位置来解决 mask 所带来的预训练与微调不匹配的问题， 这似乎比 BERT 更加优秀。</p>
<p>但从最近的实验看来，似乎又不是那么回事， XLNet 精巧的语言模型设计有没有超越 BERT， 目前学界还没有一个定论，RoBERTa 的出现似乎验证了在同等数据集下，XLNet 并不占优势， 通过精调模型参数，<strong>RoBERTa</strong> 获得了十分漂亮的结果。</p>
<p>而 XLNet 对此予以回击，又在同等条件下对比了 XLNet 与 BERT 模型， 又说明了 XLNet 效果的确要超过 BERT，emmm， 俺也不知道该相信哪个，反正我都会试试，哪个好用哪个。</p>
<p>XLNet 网上讲的很多了，我就不细说了。</p>
<h3><span id="5-预训练-中文领域">5. 预训练 + 中文领域</span></h3><p>十分推荐： <a target="_blank" rel="noopener" href="https://github.com/ymcui/Chinese-BERT-wwm">BERT-WWM</a></p>
<p>对于中文领域，分词还是分字一直是一个问题，那么，到底是选分词，还是分字，这一直是一个大问题。 </p>
<p>BERT 无疑选择了分字这条路， ERNIE 通过融入知识，其实带来了部分分词的效果，那么在预训练语言模型中，分词到底有没有用， BERT-WWM 给出了答案。</p>
<p>通过采用 mask 词的方式， 在原有的 BERT-base 模型上接着进行训练， 这其实有种 词 + 字 级别组合的方式， 我在 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/66155616">深度学习时代，分词真的有必要吗</a> 中就有提到 字级别 与 词级别之间的差别， 而预训练语言模型能很好的组织二者，的确是件大喜事。</p>
<p>而事实证明， BERT-WWM 在中文任务上的确有着优势所在，具体就不细说了，至少目前来说，我们的中文预训练语言模型有三大选择了： BERT , ERNIE, BERT-WWM。</p>
<h3><span id="6-预训练-精细调参">6. 预训练 + 精细调参</span></h3><p>通过精细调参， BERT 能够发挥出更大的威力。 RoBERTa 证明了这一点。</p>
<p>此外， RoBERTa 认为 NSP 不仅不能带来下游任务的性能提升，反而会有所损害。 RoBERTa 的出现说明 BERT 本身的还有很多潜力要挖。</p>
<p>总的来说，这篇文章依旧是个苦工活，虽创新度一般，但价值很高。</p>
<h3><span id="7-预训练-基础单元">7. 预训练+ 基础单元</span></h3><p>大多数语言模型都采用 Transformer 来作为预训练的基本单元，那么 Transformer 有没有改进的空间呢？ 必然是有的。</p>
<p>XLNet 采用 Transformerxl 作为基本单元来解决长文本问题，Transformerxl 本质上就是 Transformer + 循环机制， 这样会带来并行性上的损失。</p>
<p>相信后续还会有更多的变体来解决 Transformer 的各种问题， 如果有对 Transformer 研究十分深的同学欢迎补充一下。</p>
<h2><span id="最后">最后</span></h2><p>本来打算再多研读几篇文章再写的，但是限于精力原因（忙于秋招），只能对前段时间看的论文大致总结， 提一些自己的思路，实在是无力去找新的 Paper 了。 </p>
<p>希望9月初会下 offer 雨打我的脸啊！！！</p>
<h2><span id="reference">Reference</span></h2><p>[1]  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p>
<p>[2]  ERNIE - Enhanced Language Representation with Informative Entities</p>
<p>[3]  ERNIE - Enhanced Representation through Knowledge Integration</p>
<p>[4]  ERNIE 2.0 - A Continual Pre-training Framework for Language Understanding</p>
<p>[5]  MASS - Masked Sequence to Sequence Pre-training for Language Generation</p>
<p>[6]  RoBERTa - A Robustly Optimized BERT Pretraining Approach</p>
<p>[7]  UNILM - Unified Language Model Pre-training for Natural Language Understanding and Generation</p>
<p>[8]  XLNet - Generalized Autoregressive Pretraining for Language Understanding</p>
<h3><span id="1-ar-与-ae-语言模型">1. AR 与 AE 语言模型</span></h3><p>AR：Autoregressive Language Modeling</p>
<p>AE： Autoencoding Language Modeling</p>
<ul>
<li><p>AR 语言模型：指的是，依据前面（或后面）出现的 tokens 来预测当前时刻的 token， 代表有 ELMO， GPT 等</p>
<script type="math/tex; mode=display">
forward: p(x) = \prod_{t=1}^T p(x_t | x_{<t}) \\
backward: p(x) = \prod_{t=T}^1 p(x_t | x_{>t})</script></li>
<li><p>AE 语言模型：通过<strong>上下文信息</strong>来预测被 mask 的 token， 代表有 BERT , Word2Vec(CBOW)</p>
<script type="math/tex; mode=display">
p(x) = \prod_{x\in Mask} p(x|context)</script></li>
</ul>
<p>二者有着它们各自的优缺点：</p>
<ul>
<li><p>AR 语言模型：</p>
<blockquote>
<ul>
<li><strong>缺点：</strong>它只能利用单向语义而不能同时利用上下文信息。 ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。</li>
<li><strong>优点：</strong> 对生成模型友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。</li>
</ul>
</blockquote>
</li>
<li><p>AE 语言模型：</p>
<blockquote>
<ul>
<li><strong>缺点：</strong> 由于训练中采用了 [MASK] 标记，导致预训练与微调阶段不一致的问题。 此外对于生成式问题， AE 模型也显得捉襟见肘，这也是目前 BERT 为数不多实现大的突破的领域。</li>
<li><strong>优点：</strong> 能够很好的编码上下文语义信息， 在自然语言理解相关的下游任务上表现突出。</li>
</ul>
</blockquote>
</li>
</ul>
<h2><span id="bert-诞生之前">BERT 诞生之前</span></h2><p>NNLM， Word2vec， ELMO， GPT 1.0</p>
<h2><span id="gpt-系列">GPT 系列</span></h2><h2><span id="bert-系列">BERT 系列</span></h2><p>spanBERT, Robert</p>
<h2><span id="多任务学习">多任务学习</span></h2><p>MT-DNN</p>
<h2><span id="中文领域">中文领域</span></h2><p>bert-wwm</p>
<h2><span id="融入知识">融入知识</span></h2><p>ERNIE 系列， ERNIE-THU, LIBERT, SenseBERT， KnowBERT， BERT-MK，K-BERT，BERT-WWM，WKLM，LUKE，SemBERT，sentiLR，SKEP，KG-BERT，KEPLER</p>
<h2><span id="多语言">多语言</span></h2><p>mBERT， XLM， Unicoder，XLM-R</p>
<h2><span id="模型压缩">模型压缩</span></h2><p>ALBERT， MiniLM， DistilBERT， TinyBERT， BERT-PKD， Distilled-BiLSTM</p>
<h2><span id="文本生成">文本生成</span></h2><p>MASS， UNILM，bart</p>
<p>T5， XLNET， TransformerXL</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

  </div>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.zhihu.com/people/lirui940403">
            <span class="icon">
              <i class="fab fa-zhihu"></i>
            </span>

            <span class="label">Zhihu</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://github.com/earyantLe">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>

            <span class="label">Github</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" rel="tag"># 机器学习知识整理</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"># 深度学习自然语言处理</a>
              <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" rel="tag"># 预训练语言模型</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" rel="prev" title="如何使用预训练模型">
      <i class="fa fa-chevron-left"></i> 如何使用预训练模型
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/Recent%20Advances%20in%20Language%20Model%20Fine-tuning/" rel="next" title="Recent Advances in Language Model Fine-tuning">
      Recent Advances in Language Model Fine-tuning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>

          <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
              crossorigin="anonymous"></script>
          <ins class="adsbygoogle"
              style="display:block; text-align:center;"
              data-ad-layout="in-article"
              data-ad-format="fluid"
              data-ad-client="ca-pub-8990335822972779"
              data-ad-slot="3743679245"></ins>
          <script>
              (adsbygoogle = window.adsbygoogle || []).push({});
          </script>

          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-8990335822972779"
     data-ad-slot="3743679245"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">一文滤清预训练语言模型发展脉络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.2.</span> <span class="nav-text">1. BERT 之前</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.1 从 Word Embedding 到 ELMO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2 Fine-tuning pretraining：  GPT 的诞生</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.2.3.</span> <span class="nav-text">4. 预训练新时代：BERT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.3.</span> <span class="nav-text">2. BERT 之后</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.1.</span> <span class="nav-text">1. 预训练 + 知识图谱</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.2.</span> <span class="nav-text">2. 预训练 +  自然语言生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.  预训练 + 多任务学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.4.</span> <span class="nav-text">4. 改进语言模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.5.</span> <span class="nav-text">5. 预训练 + 中文领域</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.6.</span> <span class="nav-text">6. 预训练 + 精细调参</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.7.</span> <span class="nav-text">7. 预训练+ 基础单元</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.4.</span> <span class="nav-text">最后</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.5.</span> <span class="nav-text">Reference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.5.1.</span> <span class="nav-text">1. AR 与 AE 语言模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.6.</span> <span class="nav-text">BERT 诞生之前</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.7.</span> <span class="nav-text">GPT 系列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.8.</span> <span class="nav-text">BERT 系列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.9.</span> <span class="nav-text">多任务学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.10.</span> <span class="nav-text">中文领域</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.11.</span> <span class="nav-text">融入知识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.12.</span> <span class="nav-text">多语言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.13.</span> <span class="nav-text">模型压缩</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.14.</span> <span class="nav-text">文本生成</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Earyant"
      src="/images/me/me2.png">
  <p class="site-author-name" itemprop="name">Earyant</p>
  <div class="site-description" itemprop="description">个人技术博客，分享开发中遇到的问题，以及想学的新技术，会持续更新，可以订阅rss。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">339</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">88</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/earyantLe" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;earyantLe" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lirui940403@gmail.com" title="E-Mail → mailto:lirui940403@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/earyant" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;earyant" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/earyantLe" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;earyantLe" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/earyant" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;earyant" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i>FB Page</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/earyant" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;earyant" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/lirui940403" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;lirui940403" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://zhuyuanxiang.github.io/" title="https:&#x2F;&#x2F;zhuyuanxiang.github.io&#x2F;" rel="noopener" target="_blank">zYx.Tom</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://mofanpy.com/" title="https:&#x2F;&#x2F;mofanpy.com&#x2F;" rel="noopener" target="_blank">mofan</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://github.com/timqian/chinese-independent-blogs" title="https:&#x2F;&#x2F;github.com&#x2F;timqian&#x2F;chinese-independent-blogs" rel="noopener" target="_blank">中文独立博客推荐</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.ruanyifeng.com/blog/" title="https:&#x2F;&#x2F;www.ruanyifeng.com&#x2F;blog&#x2F;" rel="noopener" target="_blank">阮一峰</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://space.bilibili.com/1567748478" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;1567748478" rel="noopener" target="_blank">李沐</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://seekbetter.me/" title="https:&#x2F;&#x2F;seekbetter.me&#x2F;" rel="noopener" target="_blank">加菲猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.jetli.com.cn/" title="http:&#x2F;&#x2F;www.jetli.com.cn&#x2F;" rel="noopener" target="_blank">优秀博客导航</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Earyant</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">997k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">15:06</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'dc57e777868f7d0d0d6e',
      clientSecret: 'd54ae953e61c3d6fe846828c2765958a001a8654',
      repo        : 'earyant.github.io-comment',
      owner       : 'earyantLe',
      admin       : ['earyantLe'],
      id          : '8268df9955dbad62edd5cecc96040114',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>



</body>
</html>
