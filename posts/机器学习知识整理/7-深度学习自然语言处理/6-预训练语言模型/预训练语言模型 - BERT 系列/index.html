<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="true">
  <meta name="msvalidate.01" content="true">
  <meta name="yandex-verification" content="true">
  <meta name="baidu-site-verification" content="true">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.lirui.pub","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
     crossorigin="anonymous"></script>


  <meta name="baidu-site-verification" content="zqFkvU5IKZXfWGBL" />

  <meta name="description" content="BERT [1] 1 Model Architecture 2 Input Representation 3. Pre-training Tasks 4. Pre-training 训练细节 5. BERT 结果 6. 优点与缺点   RoBERTa 1. 训练数据比较： 2. dynamic masking vs static mask 3. 数据格式与NSP 4. batch size -">
<meta property="og:type" content="article">
<meta property="og:title" content="预训练语言模性-BERT系列">
<meta property="og:url" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/index.html">
<meta property="og:site_name" content="Earyant的技术博客">
<meta property="og:description" content="BERT [1] 1 Model Architecture 2 Input Representation 3. Pre-training Tasks 4. Pre-training 训练细节 5. BERT 结果 6. 优点与缺点   RoBERTa 1. 训练数据比较： 2. dynamic masking vs static mask 3. 数据格式与NSP 4. batch size -">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/BERT.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/BERT_1.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/BERT_3.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/roberta_1.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/roberta_2.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/roberta_3.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/roberta_4.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_1.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_2.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_3.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_4.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_11.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_5.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_6.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_7.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_8.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_9.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_10.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_12.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_13.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_14.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_15.png">
<meta property="og:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/T5_16.png">
<meta property="article:published_time" content="2022-03-12T15:33:22.000Z">
<meta property="article:modified_time" content="2022-03-12T15:33:22.000Z">
<meta property="article:author" content="Earyant">
<meta property="article:tag" content="机器学习知识整理">
<meta property="article:tag" content="深度学习自然语言处理">
<meta property="article:tag" content="预训练语言模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/image/BERT.png">

<link rel="canonical" href="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>预训练语言模性-BERT系列 | Earyant的技术博客</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?7b631c63a156a1da87aae10873b41f51";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Earyant的技术博客" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Earyant的技术博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">欢迎来到Earyant的技术博客，在这里我将与你分享新技术。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">88</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">19</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">341</span></a>

  </li>
        <li class="menu-item menu-item-相册">

    <a href="/gallery/" rel="section"><i class="fa fa-camera-retro fa-fw"></i>相册</a>

  </li>
        <li class="menu-item menu-item-留言板">

    <a href="/guestbook/" rel="section"><i class="fa fa-comment fa-fw"></i>留言板</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>


        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
            crossorigin="anonymous"></script>
        <ins class="adsbygoogle"
            style="display:block; text-align:center;"
            data-ad-layout="in-article"
            data-ad-format="fluid"
            data-ad-client="ca-pub-8990335822972779"
            data-ad-slot="3743679245"></ins>
        <script>
            (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      </div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/earyantLe" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.lirui.pub/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20BERT%20%E7%B3%BB%E5%88%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me/me2.png">
      <meta itemprop="name" content="Earyant">
      <meta itemprop="description" content="个人技术博客，分享开发中遇到的问题，以及想学的新技术，会持续更新，可以订阅rss。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Earyant的技术博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          预训练语言模性-BERT系列
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-12 23:33:22" itemprop="dateCreated datePublished" datetime="2022-03-12T23:33:22+08:00">2022-03-12</time>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

  
	<div>
      <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
          crossorigin="anonymous"></script>
      <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-8990335822972779"
          data-ad-slot="3743679245"></ins>
      <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
  </div>
  

    
    
    
    <div class="post-body" itemprop="articleBody">

      


        <!-- toc -->
<ul>
<li><a href="#bert-1">BERT [1]</a><ul>
<li><a href="#1-model-architecture">1 Model Architecture</a></li>
<li><a href="#2-input-representation">2 Input Representation</a></li>
<li><a href="#3-pre-training-tasks">3. Pre-training Tasks</a></li>
<li><a href="#4-pre-training-训练细节">4. Pre-training 训练细节</a></li>
<li><a href="#5-bert-结果">5. BERT 结果</a></li>
<li><a href="#6-优点与缺点">6. 优点与缺点</a></li>
</ul>
</li>
<li><a href="#roberta">RoBERTa</a><ul>
<li><a href="#1-训练数据比较">1. 训练数据比较：</a></li>
<li><a href="#2-dynamic-masking-vs-static-mask">2. dynamic masking vs static mask</a></li>
<li><a href="#3-数据格式与nsp">3. 数据格式与NSP</a></li>
<li><a href="#4-batch-size-大大大">4. batch size - 大大大</a></li>
<li><a href="#5-text-encoding">5. Text Encoding</a></li>
<li><a href="#6-训练细节">6. 训练细节</a></li>
</ul>
</li>
<li><a href="#4-t5-4">4. T5 [4]</a><ul>
<li><a href="#1-数据才是正道-c4">1. 数据才是正道 ：C4</a></li>
<li><a href="#2-text-to-text-模型归一">2. Text-to-Text： 模型归一</a></li>
<li><a href="#3-评测模型">3. 评测模型</a></li>
<li><a href="#4-baseline">4. Baseline</a></li>
<li><a href="#5-architectures">5. Architectures</a><ul>
<li><a href="#51-attention-mask-方式">5.1 Attention mask 方式</a></li>
<li><a href="#52-模型结构">5.2 模型结构</a></li>
</ul>
</li>
<li><a href="#6-unsupervised-objectives">6. unsupervised objectives</a><ul>
<li><a href="#61-high-level-approachs">6.1 High-level approachs</a></li>
<li><a href="#2-bert-mask-策略">2. bert Mask 策略</a></li>
</ul>
</li>
<li><a href="#7-pre-training-datasets">7. pre-training datasets</a><ul>
<li><a href="#71-预训练数据集的选择">7.1 预训练数据集的选择</a></li>
<li><a href="#72-预训练数据集的大小">7.2 预训练数据集的大小</a></li>
</ul>
</li>
<li><a href="#8-fine-tune">8. fine-tune</a></li>
<li><a href="#9-multi-task-learning">9. Multi-task learning</a></li>
<li><a href="#10-combining-multi-task-learning-with-fine-tuning">10. Combining multi-task learning with fine-tuning</a></li>
<li><a href="#11-scaling">11. scaling</a></li>
<li><a href="#12-t5-模型-put-it-all-together">12. T5 模型： put it all together</a></li>
</ul>
</li>
<li><a href="#questions">Questions</a><ul>
<li><a href="#1-为什么-bert-结果好于-elmo">1. 为什么 BERT 结果好于 ELMO</a></li>
<li><a href="#2-你觉得bert-有哪些需要改进的地方">2. 你觉得BERT 有哪些需要改进的地方</a></li>
</ul>
</li>
<li><a href="#reference">Reference</a></li>
</ul>
<!-- tocstop -->
<span id="more"></span>
<h2><span id="bert-1">BERT [1]</span></h2><h3><span id="1-model-architecture">1 Model Architecture</span></h3><p><img data-src="image/BERT.png" alt></p>
<p>Bert 模型结构上是一个多层，双向 Transformer encoder。</p>
<p>我们假定层数为 L， 隐层size为 H， self-attention heads 数目为A，那么有：</p>
<ul>
<li>$BERT_{BASE}$: L = 12, H=768, A= 12， 参数数量 110M, 即1.1亿</li>
<li>$BERT_{LARGE}$: L = 24, H=1024, A= 16， 参数数量 340M， 即3.4亿</li>
</ul>
<p>$BERT_{BASE}$ 与 OpenAI GPT 模型大小相同，主要是为了二者之间比较。 在下文，我们将双向Transformer称为 “ Transformer encoder”， 将单向（仅左）的Transformer称为 “Transformer decoder”。</p>
<h3><span id="2-input-representation">2 Input Representation</span></h3><p><img data-src="image/BERT_1.png" alt></p>
<ul>
<li><p>Token Embeddings：词向量，第一个单词是CLS标志，可以用于之后的分类任务</p>
</li>
<li><p>Segment Embeddings：区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务</p>
</li>
<li>Position Embeddings：<strong>和之前文章中的Transformer不一样，不是三角函数而是学习出来的</strong></li>
</ul>
<p>我们的 Input Representation 能够清楚的表示单一文本句子或句子对（如[Question, Answer]）。每个给定的token， 它的 input representation 是由 corresponding token， segment， 和 position embeddings 集合而成的。如图2所示。</p>
<p>细节如下：</p>
<ul>
<li>We use <strong>WordPiece embeddings</strong>  with a 30,000 token vocabulary. We denote split word pieces with ##.</li>
<li>We use <strong>learned positional embeddings</strong> with supported sequence lengths up to <strong>512 tokens</strong>.</li>
<li>The first token of every sequence is always the <strong>special classification embedding ([CLS]).</strong> </li>
<li>Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways.<ul>
<li>First, we separate them with a <strong>special token ([SEP]).</strong> </li>
<li>Second, we add a learned sentence A embedding to every token of the first sentence and a sentence B embedding to every token of the second sentence.</li>
</ul>
</li>
</ul>
<h3><span id="3-pre-training-tasks">3. Pre-training Tasks</span></h3><p>BERT 使用两个新的无监督预测任务来训练。</p>
<p><strong>Task 1. Mased LM  - MLM</strong></p>
<p>为了训练深度双向表征，我们随机遮蔽输入 token 的某些部分，然后预测被遮住的 token。我们将此称为“masked LM”（MLM，类似于我们的完形填空）。在这种情况下，对应于遮蔽 token 的最终隐藏向量会输入到 softmax 函数中，并如标准 LM 中那样预测所有词汇的概率。在所做的所有实验中，我们随机遮住了每个序列中 15% 的 WordPiece token。</p>
<p>虽然该方法能够获得双向预训练模型，但该方法有两个弱点：</p>
<ol>
<li>训练与微调阶段的不一致性，因为训练阶段采用了 [MASK] 而 fine-tune 阶段并没有。 为了减轻该问题， we do not always replace “masked” words with the actual [MASK] token. 具体做法如下：</li>
</ol>
<blockquote>
<p> 假如我们有一句话， my dog is hairy ， 被选中的词为hairy，数据生成器并不总是将hairy替换为[MASK]，此时的过程如下：</p>
<ul>
<li>80% 情况下： 用[MASK] 替换 hairy</li>
<li>10% 情况下： 随机选一个词如apple 来替换hairy</li>
<li>10%: 不改变这句话</li>
</ul>
</blockquote>
<ul>
<li>only 15% of tokens are predicted in each batch,  which suggests that more pre-training steps may be required for the model to converge.</li>
</ul>
<p><strong>Task 2. Next Sentence Prediction - NSP</strong></p>
<p>语言模型不能获取两个句子之间的关系，因此我们预训练了一个  binarized next sentence prediction  task， 该任务可以从任意单语语料库中轻松生成。 </p>
<p>具体来说，我们选定一个句子A，B作为预训练样本，B有50%的可能是A的下一句，也有50%的可能是语料库的随机句子。举例而言：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- Input: [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</span><br><span class="line">- Label: IsNext</span><br><span class="line"></span><br><span class="line">- Input: [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</span><br><span class="line">- Label: NotNext</span><br></pre></td></tr></table></figure>
<h3><span id="4-pre-training-训练细节">4. Pre-training 训练细节</span></h3><p>预训练数据集采用 <strong>BooksCorpus（800M）</strong>和 <strong>English Wikipedia</strong> 语料。</p>
<p>为了生成每个训练输入序列，我们从语料库中抽取两段文本，我们将其称为“句子”，尽管它们通常比单个句子长得多(但也可以短一些)。第一句记做 A embedding，第二句记做 B embedding。 50% 的情况下B是A的真正后一句话， 50%情况下B是一个随机句子。两个句子的总长度 &lt;= 512 个 tokens。</p>
<p>我们设 batch_size = 256 ，则有256 sequences * 512 tokens = 128,000 tokens/batch， 训练步数为1000000步，大约33亿 word corpus 中的40个epoch。优化算法采用 Adam， 学习率设为 1e-4， $\beta_1 = 0.9， \beta_2 = 0.999$，  L2 权重衰减为 0.01。 在所有层使用dropout， 概率为0.1. 我们采用gelu激活函数而非relu。 训练损失为  the sum of the mean masked LM likelihood and mean next sentence prediction likelihood.</p>
<ul>
<li>$BERT_{BASE}$ 在 16个TPU芯片上进行训练</li>
<li>$BERT_{LARGE}$ 在 64个TPU 芯片上进行训练</li>
</ul>
<h3><span id="5-bert-结果">5. BERT 结果</span></h3><p><img data-src="image/BERT_3.png" alt></p>
<h3><span id="6-优点与缺点">6. 优点与缺点</span></h3><p><strong>优点：</strong></p>
<p><strong>缺点：</strong></p>
<ul>
<li>采用 mask 机制，导致预训练与微调阶段的不一致</li>
<li>每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢</li>
</ul>
<h2><span id="roberta">RoBERTa</span></h2><p>RoEBERTa 其实是对BERT的进一步探索，在同等参数量的情况下，探索了 BERT 关于数据量，模型架构，训练任务等问题，主要包含以下几个方面：</p>
<h3><span id="1-训练数据比较">1. 训练数据比较：</span></h3><p>BERT采用了BOOKCORPUS 和英文维基百科， 总共16GB。而 RoBERTa采用了BOOKCORPUS + 英文维基百科+ CC-NEWS+OPENWEBTEXT+STORIES， 总共160GB。</p>
<p>Roberta 于bert 都采用 512 个token 作为序列长度，但与bert不同的是， robert 不会随机掺杂一些短句，这意味着 roberta 采用的都是长句。</p>
<h3><span id="2-dynamic-masking-vs-static-mask">2. dynamic masking vs static mask</span></h3><p>在同等参数量级（bert-base）情况下，比较动态mask与静态mask的差别。</p>
<ul>
<li><strong>静态mask：</strong>Bert 在准备训练数据时，每个样本只会进行一次随机mask，每个epoch都重复使用，后续的每个训练步都采用相同的mask。</li>
<li><strong>修改版静态mask：</strong> 在预处理时将数据集拷贝10次，每份数据采用不同的mask。</li>
<li><strong>动态mask</strong>：不在预处理时进行mask，而是在每次向模型输入时动态生成mask</li>
</ul>
<p><img data-src="image/roberta_1.png" alt></p>
<p>从上表可以看出，修改版静态mask的确能够略微提升结果，而修改版静态mask结果与动态mask相差无几。</p>
<h3><span id="3-数据格式与nsp">3. 数据格式与NSP</span></h3><p>本节探讨输入训练数据的格式以及NSP任务的必要性。主要通过四个对比实验</p>
<p><img data-src="image/roberta_2.png" alt></p>
<ul>
<li><strong>Segment-pair + NSP：</strong>与bert一样。输入包含两个 segment，这两个segment可能会来自同一个文档或不同文档，两个segment 的token 数均小于 512，预训练任务包含 MLM 与 NSP。</li>
<li><strong>Sentence+pair + NSP：</strong>输入包含两个 sentence，两个句子可能来自同一文档或不同文档，两个句子 token 数均少于 512。预训练任务包含 MLM 与 NSP。</li>
<li><strong>Full-sentences：</strong>输入只有一部分，来自同一个文档或不同文档的连续句子，token总数不超过512。输入可能跨越文档边界，如果跨文档，则在上一个文档末尾添加文档边界token。不包含NSP任务。</li>
<li><strong>Doc-sentences：</strong>输入只有一部分，输入来自同一个文档的连续句子，token总数不超过512。预训练不包含 NSP 任务。</li>
</ul>
<p>通过四个对比实验我们发现：</p>
<ul>
<li>Segment-pair 较好于 sentence-pair，可能是因为 segment 能够学习到长距离依赖关系。</li>
<li>Doc-sentences 几乎在所有任务中表现最佳，这意味着 NSP 任务没有什么用</li>
<li>Doc-sentences 略好于 Full-sentences。</li>
</ul>
<h3><span id="4-batch-size-大大大">4. batch size - 大大大</span></h3><p>以往的神经机器翻译研究表明，采用非常大的mini-batches进行训练时候，搭配适当提高学习率既可以提高优化速度，又可以提高最终任务性能。</p>
<p>Bert 采用 batch-size 256，训练了1M 步。 此处比较了在保证总步数（batch_size * 步数）不变的情况下，增大 batch_size 所带来的变化。</p>
<p><img data-src="image/roberta_3.png" alt></p>
<p>通过上表可以发现，提高 batch_size，在总步数不变的情况下，增大学习率，最终获得的优化效果相差无几。</p>
<h3><span id="5-text-encoding">5. Text Encoding</span></h3><p>BERT 采用 wordpiece 来进行分词</p>
<p>roberta 采用BPE 来分词</p>
<h3><span id="6-训练细节">6. 训练细节</span></h3><p><img data-src="image/roberta_4.png" alt></p>
<p>通过上表我们看到，增加数据量带来的效果是显而易见的，而训练时间越长，获得的结果越好，但训练到一定程度，增益已经非常缓慢了。</p>
<h2><span id="4-t5-4">4. T5 [4]</span></h2><p>T5 本质上就是解决了人们对如何才能训练一个好的 PTM 的疑问。</p>
<h3><span id="1-数据才是正道-c4">1. 数据才是正道 ：C4</span></h3><p><strong>T5再次证明了数据的力量，没有什么是数据搞不定的，如果搞不定，那么再加点。</strong></p>
<p>搞数据一直是一个工作量比较大的事情，在实际业务中也必不可少，C4 的构建过程对实际还是有参考价值的。整个构建过程如下：</p>
<blockquote>
<p><strong>首先，</strong>从Common Crawl 上获取了大量的文本，然后经过清洗后获得了750GB的高质量数据文本。 作为一名算法工程师，清洗数据在所难免，这里强调一下数据清洗的重要性。</p>
</blockquote>
<p>文章中提到的清洗方法值得学习一下：</p>
<ul>
<li>只保留以标点符号结尾的句子，这些表点符号包括句号，感叹号，问号以及引号。</li>
<li>删除任何带有“淫秽，色情，暴力”等词的句子，这些词可以可以从 <a target="_blank" rel="noopener" href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words">List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</a> 查到。</li>
<li>删除所有带有JavaScript这一词的行。</li>
<li>包含 “lorem ipsum” 的页面。</li>
<li>删除所有带花括号（编程语言）的页面。</li>
<li>对于连续三句话重复出现的情况，只保留一个。</li>
</ul>
<h3><span id="2-text-to-text-模型归一">2. Text-to-Text： 模型归一</span></h3><p>这部分算是本文真正的创新点之一，即将所有任务归结到一个大的框架下，即：Text-to-Text Transformer，其本质上是一个采用 标准Transformer（Attention is all you need） 来搭建的 Encoder-Decoder 框架，统一框架这就意味着对于所有的NLP任务，都采用一样的模型，一样的损失函数，一样的解码过程。其实本质上还是 Seq2Seq， 换汤不换药。</p>
<p>我们知道，NLP任务中包含多种任务，其实主要是生成任务，分类任务以及回归任务，Text-To-Text 对各种任务提出了统一的框架。</p>
<ul>
<li><p><strong>生成任务：</strong> 直接通过Encoder-Decoder生成句子即可。思路跟传统的Encoder-Decoder一样。</p>
</li>
<li><p><strong>分类任务</strong>： 直接生成对应的标签，如情感分类任务，可以在句子前面加上：”sentiment:”， 那么输出就会是 “negative” 或 “positive”。</p>
</li>
<li><p><strong>回归任务：</strong> 这里的处理感觉就没有大道至简的感觉，它将回归任务转化为了分类任务，比如1-5分，以 0.2 为间隔，划分为25个类，然后预测类别。感觉这里的处理有点为了归一而归一的意思。</p>
<p>举个简单的例子，假如我想要预测北京房价，如果是人类预测，他首先判断，是在千万级别判断，然后得出在1kw以下，然后在百万级别判断，得出在200-300w之间，然后在在十万级别判断，是在20-30w之间，然后在万级别判断，是在5w-6w之间，以此类推，直到一定的精度。</p>
<p>而如果模型要做的话，难道不应该直接生成吗？从[0-9.]中在合适的时间选择，这样才符合回归任务的本质吧。</p>
</li>
</ul>
<p>模型的细节其实也值得探讨一下，主要包含以下几个方面，这些细节完全参照 BERT：</p>
<ul>
<li><strong>Layer Normalization：</strong> 在每个 Block 的输入前使用</li>
<li><strong>残差连接：</strong> 将每个 Block 的输入与输出加起来</li>
<li><strong>Dropout：</strong> 用于 feed-forward 网络， 残差连接， attention weights， 以及整个stack 的输入输出。</li>
<li><strong>Relative Position Embedding：</strong> 与之前采用 <strong>sinusoidal position signal</strong>（attention is all you need 中使用的) 或 <strong>学习的 position embedding</strong>（BERT中使用的）不同， 本文中采用<strong>相对位置编码</strong>。</li>
</ul>
<h3><span id="3-评测模型">3. 评测模型</span></h3><p>为了评估模型在各个任务上的表现， 文章将模型适配到各个主流数据集上，主要包含四大任务： 机器翻译，问答，文本摘要以及文本分类：GLUE， SuperGLUE， CNN/Daily Mail， SQuAD， WMT English to German, French, and Romanian translation。</p>
<p>为了在这些数据集上训练，且采用统一的框架 Text-to-Text， T5 参考了多任务学习时的做法，对每一个数据集都定义了特别的输入。 </p>
<ul>
<li><p>对于翻译，如果是 English-to-German，那么格式为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input: translate English to German: That is good.</span><br><span class="line">output: Das ist gut.</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于分类：如针对 MNLI 数据集，则输入输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input: mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.</span><br><span class="line">output: entailment</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>其他几个典型的输入输入输出如下图所示，这部分就不赘述了，其实没啥意思。</p>
<p><img data-src="image/T5_1.png" alt></p>
<h3><span id="4-baseline">4.  Baseline</span></h3><ul>
<li><strong>Model:</strong> 采用标准 Encoder-Decoder Transformer（Attention is all you need），目的是能够在生成效果方面获得一些显著效果。Model size： 模型的 encoder， decoder size 接近，都采用跟 BERT base 一样多的 block（12个）。每个block的前馈神经网络输出纬度为$d<em>{ff}=3072$； attention机制的每个纬度为$d</em>{kv}=64$，采用12个head；sub-layers 与 embedding是的纬度为 $d_{model} = 768$。整个model 有 220 million 参数，是BERT-base的两倍。dropout = 0.1</li>
<li><strong>Training：</strong>损失函数采用标准的极大似然估计；优化器选择 AdaFactor；在test时，采用 greedy decoding；步数：$2^{19}=524288$；sequence length： 512； batch size 128。优化时的学习率衰减，在前 $10^4$采用 0.01 的学习率，在后面采用 $\frac{1}{\sqrt{max(n,k}}$ 对学习率进行衰减；其中 n是当前的 training iteration， k 是 warm-up steps（10^4）。</li>
<li><strong>fine-tune</strong>：步数： $2^{18}=262144$，batch size：128， sequence len：512；learning rate：0.001；每5000 steps 存 checkpoints。</li>
<li>Vocabulary： SentencePiece + WordPiece。</li>
<li><strong>Unsupervised objective</strong>：</li>
</ul>
<h3><span id="5-architectures">5. Architectures</span></h3><p>模型结构中主要涉及两个方面： <strong>模型架构</strong> 与 <strong>Attention mask 方式</strong>。</p>
<h4><span id="51-attention-mask-方式">5.1 Attention mask 方式</span></h4><p>在介绍这三种模型结构时，先来介绍 Attention 中的三种mask方式，mask方式的不同会极大的影响模型的效果，三种mask方式如下图所示：</p>
<p><img data-src="image/T5_2.png" alt></p>
<p>三种mask 方式分别为：</p>
<ul>
<li>Fully-visible： 同时看上下文的信息，典型的就是BERT了。</li>
<li>Causal：很常见的mask机制，分为从左到右，从右到左两种，当前点仅能看到之前的信息，而看不到之后的信息，具体可以参见《Attention is all you need》decoder 的输入部分。 同时， GPT 也是采用的这种方式，一般生成式语言模型都会采用这种方式：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70663422">Bert 之后：预训练语言模型与自然语言生成</a></li>
<li>Causal with prefix：典型的 UNILM 中的 Seq-to-Seq LM 就是这种做法。</li>
</ul>
<h4><span id="52-模型结构">5.2 模型结构</span></h4><p>模型架构中，主要有 <strong>Encoder-Decoder，Language model，Prefix LM</strong> 这三种，如下图所示：</p>
<p><img data-src="image/T5_3.png" alt></p>
<ul>
<li><strong>Encoder-Decoder：</strong>  encoder 采用<strong>fully-visible</strong> attention mask，decoder 中采用 <strong>causal</strong> attention mask。其实本质上与 “attention is all you need” 结构差不多。</li>
<li><strong>Language model：</strong> 该结构相当于上面的 decoder 部分，典型的如 GPT 就是，这里其实就是延续的 GPT  的思路。mask 方式当然是 Causal 了。</li>
<li><strong>Prefix LM：</strong> full-visible 与 causal 都有着各自的缺陷，见：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79371603">就最近看的paper谈谈预训练语言模型发展</a>。Causal with prefix算是二者的均衡。</li>
</ul>
<p><strong>模型结构的最终实验结果如下：</strong></p>
<p><img data-src="image/T5_4.png" alt></p>
<p>对于目标函数的选择，这里比较了 Denoising（BERT式）以及 LM（GPT式）两种方法。从上标中我们可以得出以下结论：</p>
<ul>
<li>Encoder-Decoder + Denoising 效果是最好的。</li>
<li>Encoder 与 Decoder 之间共享参数能够获得相近的最好效果。</li>
<li>Encoder与Decoder层数减半会损害结果。</li>
<li>ALBERT 中发现共享 self-attention 参数能够降低参数量，但并不会带来很大的损害，参见：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/92849070">ALBERT 告诉了我们什么？</a></li>
<li>参数共享的Encoder-Decoder performance 优于 decoder-only prefix LM ，这说明 Encoder-Decoder 架构是有益的。</li>
<li>Denoising 效果总是优于 LM 的效果。</li>
</ul>
<h3><span id="6-unsupervised-objectives">6. unsupervised objectives</span></h3><p><img data-src="image/T5_11.png" alt></p>
<p>文章按照上图中的顺序，从左到右依次探讨。</p>
<h4><span id="61-high-level-approachs">6.1   High-level approachs</span></h4><p>首先是 High-level approachs， 此处主要探讨的是，几种常见不同的目标函数的结果，主要包括以下三种，它们的输入输出如上图所示：</p>
<p><img data-src="image/T5_5.png" alt></p>
<p>图中，<code>&lt;M&gt;</code> 表示 <code>[MASK]</code> 标志</p>
<ul>
<li><strong>Prefix language modeling：</strong>该方法将句子分为两截，一部分作为输入到Encoder 中，另一部分作为Decoder的输出。</li>
<li><strong>masked language modeling(MLM)：</strong> 就是BERT那种形式。随机替换15%的token， 其中 90% 替换为[MASK]标志，10% 替换为随机token。</li>
<li><strong>deshuffling objective：</strong> 该方法会shuffle 句子中的token，并要求预测原始的句子。</li>
</ul>
<p><img data-src="image/T5_6.png" alt></p>
<p>三个结果的比较如上图所示， 我们看到， BERT-Style 的结果往往是最好的， prefix language modeling objective 能获得相近的结果。</p>
<h4><span id="2-bert-mask-策略">2. bert Mask 策略</span></h4><p>本节第二部分就是进一步探讨， 在 BERT-Style 内部，哪种方式 mask 方式是最好的呢？</p>
<p><img data-src="image/T5_7.png" alt></p>
<p>首先，第一个探讨的问题就是 mask 策略，如上图所示：</p>
<ul>
<li><strong>mask token：</strong> mask 掉 token，将替换的token 换成 [MASK]（类似 BERT）</li>
<li><strong>replace span：</strong>为了提高计算效率。将句子中span的token替换为其他的token（类似 SpanBERT）</li>
<li><strong>drop tokens：</strong> 直接丢弃掉 tokens。</li>
</ul>
<p><img data-src="image/T5_8.png" alt></p>
<p>这三种方法的结果如上图所示，可以得出， Replace span的方法是最好的。</p>
<p>接下来就探讨，<strong>应该mask 掉多少百分比的文本呢？</strong>如下图所示，最终发现 15% 的效果是最好的。</p>
<p><img data-src="image/T5_9.png" alt></p>
<p>最后， 前面得出要采用 replace span 方法，那么 <strong>span 的长度应该采用多长呢</strong>？结果如上图所示， 最终发现 3 的长度是最好的。</p>
<p><img data-src="image/T5_10.png" alt></p>
<h3><span id="7-pre-training-datasets">7. pre-training datasets</span></h3><p>本节探讨预训练数据集的重要性，主要分为两个部分： <strong>数据集的选择</strong>以及<strong>数据集大小</strong>的选择。</p>
<h4><span id="71-预训练数据集的选择">7.1 预训练数据集的选择</span></h4><p>在数据集的选择中，主要比较了 <strong>C4</strong>， <strong>Unfiltered C4</strong>（未经过过滤的C4文本），<strong>RealNews-like</strong>，<strong>WebText-like</strong>，<strong>Wikipedia</strong>，<strong>Wikipedia + Toronto Books Corpus</strong> 这几个数据集， 其结果如下：</p>
<p><img data-src="image/T5_12.png" alt></p>
<p>上表可以发现：</p>
<ul>
<li>Unfiltered C4 是未经过上述策略过滤的数据，与 C4 比较就可以发现， C4的效果明显提升，这再次验证了一个高质量数据集的重要性。</li>
<li>Wikipedia + TBC 数据集在 SuperGLUE 上的表现要比 C4好，这说明预训练数据集与任务数据集之间的相关性是十分重要的。即 pre-training on in-domain unlabeled data can improve performance on downstream tasks. 但需要注意的是单领域的数据集往往较小，因此可能会产生一些问题。</li>
</ul>
<h4><span id="72-预训练数据集的大小">7.2 预训练数据集的大小</span></h4><p>此处主要探讨两个问题：<strong>数据集的大小</strong>以及<strong>样本重复</strong>所带来的影响。我们选择的 Full Dataset 的大小为 $2^{35} B$ tokens，只是 C4 的一个子集。 实验结果如下：</p>
<p><img data-src="image/T5_13.png" alt></p>
<p><img data-src="image/T5_14.png" alt></p>
<p>从实验结果中我们可以看出：</p>
<ul>
<li>随着数据集 size 的减小， performance 在降低。通过 Training loss 曲线，随着数据集size的减小， training loss，这说明存在一定的过拟合现象，模型对小的数据集存在 memorize 现象。</li>
<li>当样本重复64次时，所带来的影响是有限的，这说明一定程度的预训练数据重复并不会带来损害。</li>
</ul>
<h3><span id="8-fine-tune">8. fine-tune</span></h3><p>本节讨论了用于<strong>如何在 Text-to-Text 上使用微调手段。</strong>主要有三种手段：</p>
<ul>
<li>微调 endocer-decoder 的所有参数。 </li>
<li><strong>adapter layers：</strong> 在decoder 外添加一层 dense-Relu-dense的 前馈网络，微调该前馈网络而不是微调所有参数。 同时， 该前馈网络的维度 d 的选择也十分重要，因此作者比较了多个维度。</li>
<li><strong>gradual unfreezing：</strong> 即随着时间的推移，越来越多的参数参与训练。最开始，只有最后一层开始训练，然后，随着时间推移，慢慢加入前面层的参数，直到所有的参数都参与训练。</li>
</ul>
<p><img data-src="image/T5_15.png" alt></p>
<p>从上图中我们发现：</p>
<ul>
<li>微调所有参数往往能够获得最好的结果</li>
<li>对于 low-resource 的任务如 SQuAD， 即使是小的 d 也能工作的很好</li>
<li>对于 High-resource 任务往往需要更大 的 d</li>
<li>对于微调来说， adapter layers 是一个不错的手段，能够很好的权衡 performance 与 训练性能， 且 d 的选择也颇为重要。</li>
</ul>
<h3><span id="9-multi-task-learning">9. Multi-task learning</span></h3><h3><span id="10-combining-multi-task-learning-with-fine-tuning">10. Combining multi-task learning with fine-tuning</span></h3><h3><span id="11-scaling">11. scaling</span></h3><p><img data-src="image/T5_16.png" alt></p>
<p>本节主要讨论了几个 <strong>Scaling 策略</strong>所带来的影响。主要涉及到的策略有：增大模型size， 增加 training steps， 增大 batch size。需要注意的是，<strong>当增加 trainging steps与 batch size 时，也要相应的增加训练数据。</strong>那其实问题就回到了，增加数据与增大模型所带来的效果增益。</p>
<p>从结果来看，<strong>增加数据与增大模型对performance 都是有影响的</strong>，且增大模型所带来的增益更大，且这两种 scaling 同时使用效果更佳。</p>
<h3><span id="12-t5-模型-put-it-all-together">12.  T5 模型： put it all together</span></h3><h2><span id="questions">Questions</span></h2><h3><span id="1-为什么-bert-结果好于-elmo">1. 为什么 BERT 结果好于 ELMO</span></h3><ol>
<li>Transformer 抽取特征的能力强于 LSTM</li>
<li>ELMO 中采用直接拼接进行多层向量融合的方式偏弱</li>
<li>BERT 参数量远多于 ELMO</li>
</ol>
<h3><span id="2-你觉得bert-有哪些需要改进的地方">2. 你觉得BERT 有哪些需要改进的地方</span></h3><h2><span id="reference">Reference</span></h2><p>[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p>
<p>[2] RoBERTa: A Robustly Optimized BERT Pretraining Approach</p>
<p>[4] T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

  </div>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://www.zhihu.com/people/lirui940403">
            <span class="icon">
              <i class="fab fa-zhihu"></i>
            </span>

            <span class="label">Zhihu</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="https://github.com/earyantLe">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>

            <span class="label">Github</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" rel="tag"># 机器学习知识整理</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" rel="tag"># 深度学习自然语言处理</a>
              <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" rel="tag"># 预训练语言模型</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20-%20ERNIE%20%E7%B3%BB%E5%88%97/" rel="prev" title="预训练语言模性-ERNIE系列">
      <i class="fa fa-chevron-left"></i> 预训练语言模性-ERNIE系列
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/7-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/6-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%A6%82%E4%BD%95%E6%9B%B4%E5%A5%BD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AABERT/" rel="next" title="如何更好的与训练一个BERT">
      如何更好的与训练一个BERT <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>

          <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
              crossorigin="anonymous"></script>
          <ins class="adsbygoogle"
              style="display:block; text-align:center;"
              data-ad-layout="in-article"
              data-ad-format="fluid"
              data-ad-client="ca-pub-8990335822972779"
              data-ad-slot="3743679245"></ins>
          <script>
              (adsbygoogle = window.adsbygoogle || []).push({});
          </script>

          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8990335822972779"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-8990335822972779"
     data-ad-slot="3743679245"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">BERT [1]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.</span> <span class="nav-text">1 Model Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.2.</span> <span class="nav-text">2 Input Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.</span> <span class="nav-text">3. Pre-training Tasks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.4.</span> <span class="nav-text">4. Pre-training 训练细节</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.5.</span> <span class="nav-text">5. BERT 结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.6.</span> <span class="nav-text">6. 优点与缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">RoBERTa</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.</span> <span class="nav-text">1. 训练数据比较：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.2.</span> <span class="nav-text">2. dynamic masking vs static mask</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.3.</span> <span class="nav-text">3. 数据格式与NSP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.4.</span> <span class="nav-text">4. batch size - 大大大</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.5.</span> <span class="nav-text">5. Text Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.6.</span> <span class="nav-text">6. 训练细节</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">4. T5 [4]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.1.</span> <span class="nav-text">1. 数据才是正道 ：C4</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.2.</span> <span class="nav-text">2. Text-to-Text： 模型归一</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.3.</span> <span class="nav-text">3. 评测模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.4.</span> <span class="nav-text">4.  Baseline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.5.</span> <span class="nav-text">5. Architectures</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.5.1.</span> <span class="nav-text">5.1 Attention mask 方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.5.2.</span> <span class="nav-text">5.2 模型结构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.6.</span> <span class="nav-text">6. unsupervised objectives</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.6.1.</span> <span class="nav-text">6.1   High-level approachs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.6.2.</span> <span class="nav-text">2. bert Mask 策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.7.</span> <span class="nav-text">7. pre-training datasets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.7.1.</span> <span class="nav-text">7.1 预训练数据集的选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">3.7.2.</span> <span class="nav-text">7.2 预训练数据集的大小</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.8.</span> <span class="nav-text">8. fine-tune</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.9.</span> <span class="nav-text">9. Multi-task learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.10.</span> <span class="nav-text">10. Combining multi-task learning with fine-tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.11.</span> <span class="nav-text">11. scaling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">3.12.</span> <span class="nav-text">12.  T5 模型： put it all together</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">Questions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.1.</span> <span class="nav-text">1. 为什么 BERT 结果好于 ELMO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.2.</span> <span class="nav-text">2. 你觉得BERT 有哪些需要改进的地方</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Earyant"
      src="/images/me/me2.png">
  <p class="site-author-name" itemprop="name">Earyant</p>
  <div class="site-description" itemprop="description">个人技术博客，分享开发中遇到的问题，以及想学的新技术，会持续更新，可以订阅rss。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">341</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">88</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/earyantLe" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;earyantLe" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lirui940403@gmail.com" title="E-Mail → mailto:lirui940403@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/earyant" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;earyant" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/earyantLe" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;earyantLe" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/earyant" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;earyant" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i>FB Page</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/earyant" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;earyant" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/lirui940403" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;lirui940403" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://zhuyuanxiang.github.io/" title="https:&#x2F;&#x2F;zhuyuanxiang.github.io&#x2F;" rel="noopener" target="_blank">zYx.Tom</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://mofanpy.com/" title="https:&#x2F;&#x2F;mofanpy.com&#x2F;" rel="noopener" target="_blank">mofan</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://github.com/timqian/chinese-independent-blogs" title="https:&#x2F;&#x2F;github.com&#x2F;timqian&#x2F;chinese-independent-blogs" rel="noopener" target="_blank">中文独立博客推荐</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.ruanyifeng.com/blog/" title="https:&#x2F;&#x2F;www.ruanyifeng.com&#x2F;blog&#x2F;" rel="noopener" target="_blank">阮一峰</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://space.bilibili.com/1567748478" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;1567748478" rel="noopener" target="_blank">李沐</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://seekbetter.me/" title="https:&#x2F;&#x2F;seekbetter.me&#x2F;" rel="noopener" target="_blank">加菲猫</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.jetli.com.cn/" title="http:&#x2F;&#x2F;www.jetli.com.cn&#x2F;" rel="noopener" target="_blank">优秀博客导航</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Earyant</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">15:25</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'dc57e777868f7d0d0d6e',
      clientSecret: 'd54ae953e61c3d6fe846828c2765958a001a8654',
      repo        : 'earyant.github.io-comment',
      owner       : 'earyantLe',
      admin       : ['earyantLe'],
      id          : 'f1b73f4d5a75a1388d949e510857ed11',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>



</body>
</html>
